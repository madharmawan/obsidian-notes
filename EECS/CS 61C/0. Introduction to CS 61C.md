# 0. Introduction
This note is an introduction to CS 61C. 
Get back to [[toc CS61C]].

### Welcome

Welcome to CS 61C! My name is Matthew Dharmawan, the author of these notes. I took this class in Summer 2022, and I really enjoyed learning the subject. Unlike CS 61A and 61B, this class focuses on low-level concepts; the things you took for granted in 61A and 61B are now something you need to understand and learn why it works. This down to the detail of "how can you read data from an array? Well you have to look at the address it is in, and look in the cache. You designate these bits for specifying blocks in the cache, and those bits for what part of the block you want..." You get the idea. The way that I want to approach this is starting from a high level, and understanding tools that allow us to get lower and lower.

### Overview
  
I always like to give a preview on what is to come to see why we learn certain things to help set up for important concepts.

1. Number Representation: The first major tool we will understand is number representation: your bits, bytes, twos-complement, hexadecimal, bias, floating point representations. These will be extremely useful and you'll see it throughout the rest of the course. When we talk about RISC-V Instruction Formats, Datapath, Caching, Virtual Memory, Dependability -- you can see number representation will seep into a lot of concepts we will talk about.

2. C Programming: After that, we start from a very high-level: C (C is a low-level language, so it is not really high, but high-level relative to the rest of the concepts we will cover). If it is your first time looking at C, you'll see the syntax is very similar to Java. However, there are major differences that is taken for granted in Java that you don't have in C. For example, you have to request memory before you use it. In Java, the memory request is done under-the-hood for you, but in C, you have to explicitly use calls like `malloc`, `calloc`, and `realloc`, as well as specify how many bytes of data you want to allocate based on the situation given. What is even more detailed is that you also have to be your own garbage collector and `free` these allocated parts of memory, otherwise you'll be left with memory leaks, and that is bad.

3. Memory: Then we will take a brief sidestep from C and look into stack, heap, static, and data. We will see when data is stored on the stack vs. the heap vs. static vs. data, and why it should be like this. Afterwards, we will talk about how data gets aligned in memory based on its size to make for easier accessing.

4. More C Programming: Ok, now we go back to C programming and wrap-up things we didn't really talk about - structs and unions. We will finish off with doig a few exercises in C combining everything, then we won't touch C until nearing the end of the course.

5. RISC-V Programming: Now it is time to go one level deeper. We know C is a compiled language, but what does it mean to be complied? C will get converted to an assembly language. In our case, that assembly language is RISC-V! RISC-V is a reduced instruction set computer, meaning that all operations are as simple as possible; if there is such an operation that could be broken down into two simpler instructions, then we don't include that more complicated one. Here, we will learn how to deal with managing register data, and using addresses (pointers) to access and store data. Programming in RISC-V can feel a bit tedious, but it is right above the step of programming in bits (and you don't want to program in bits).

6. Assembly Calling Convention: Because of the nature of the 32 registers in RISC-V, we will need to establish some sort of calling convention. How can we organize registers such that each main category can do a specific thing? How do we deal with registers once we call a function? You can think of registers as "global variables" so if you call a function that alters s8, it will alter s8 in the function that originally called it, so hopefully you don't use it again! However, we can bypass this problem with a calling convention.

7. RISC-V Instructions and Bits: Ok, now let's go another step lower. Let's convert RISC-V instructions into bits. Don't worry, you won't be coding loops and conditionals with bits, it's more like understanding how to categorize RISC-V instructions and represent them in 32 bits (or 64 bits, depending on architecture).

8. CALL: If we were to go another step lower, we would learn about linkers and loaders. After learning about RISC-V, we will take a moment to talk about the entire process, from Compiler to Assembler to Linker to Loader. What do each step do such that a program can run on your machine?

9. Boolean Logic and Synchronous Digital Systems: Now we will begin preparing some prerequisite knowledge to build your own CPU that can understand and execute all the RISC-V base-set instructions. To do that, we will have to look into boolean logic (for learning about gates like AND, OR, XOR, MUX), and Synchronous Digital Systems (for learning about registers and clocks.)

10. Finite State Machines: After learning about SDS, another topic that deals with determining state are Finite-State Machines, which tell you how to proceed with inputs and outputs. You can create FSMs that tell you about the bits, such as if a 1 appears 3 times in a row, output a 1, otherwise, output a 0. We will look into several important examples of FSMs here. FSMs are important for visualizing cache coherency.

11. Datapath: Okay, now we have a lot of prerequisite knowledge. Let's combine it all together and talk about datapath, which is the sequence and combination of logic that will help us interpret RISC-V code. We will talk about control logic, and how each part of the control logic allows us to determine how to proceed with the instruction given, such as if we want to write or read, use an immediate or not, how to handle branching, etc. We will come to understand the following diagram. 

12. Operating Systems: Let's take another sidestep and learn about the Operating System, the program responsible for making sure your computer doesn't crash when there is one error. Understanding how OS is able to keep track of several processes at once and manage memory is an important topic throughout the course, as we have seen. 

13. Pipelining: It takes a while for one single instruction to run on our CPU. How about we pipeline the datapath? Pipelining is the idea where we can run instructons somewhat at the same time, such as when an instruction is being decoded, at the same time, the next instruction is being fetched. When an instruction is executed, another will be decoded, and another will be fetched. However, pipelining introduces structural, data, and control hazards, but we will see how to work around that given what we already have.

14. Data and Thread Level Parallelism: With the introduction of pipelining, we introduced a huge idea in computer architecture - parallelism: The idea of running several things at once rather than in sequence. Here, we will take a look at both Data-Level Parallelism (DLP) using vectorization and SIMD instructions, then look at Thread-Level Parallelism (TLP) using OpenMP to introduce threads that run simultaneously. 

15. Caches: Now, it is time to introduce a huge topic in this course that takes advantage of a lot of what we have been building up to: Caches. Caches are the idea of making accessing memory quicker by bringing in blocks of data from disk into a cache. The idea is if you took data from this block, it is highly likely you might take a look at that access again, or at the very least data around that initial access. Of course, accessing data this fast can be quite expensive, so we will have to take a look at how a cache is able to evict blocks when it is full, and learn about the several different tyeps of caches (Fully Associative, Direct-Mapped, N-Way Associative). We will also see how exactly we can bring blocks in data using T/I/O bits, and figure out what type of miss or hit will occur based on the state of the cache and the request bits of TIO. In addition, we will introduce the idea of multi-level caching, and how that might help us do more with caches.

16. Cache Coherency: This is where FSMs make a return, now in combination with parallelism and caches! 

17.  Virtual Memory: Another huge topic is the idea of virtual memory. 

18. Dependability: Now that we all have these systems in place, there is a very small chance that a bit will flip due to a cosmic ray hitting the device and flipping a bit. Now if there is no system in place to detect that bit, the whole disk could fail, which is terrible. How can we go about knowing if a sequence of bit has an error? This is where Error Correcting Codes come into the scene, allowing us to pinpoint exactly when a bit should not be in its current state. How might we do that? Let's look at parity, the number of 1's in a bitstring. If we can group specific combinations of bits and represent its parity with another bit, it seems like if we are creative enough, we can tell which bit in the bitstring is flipped (including the parity bits themselves). Afterwards, we will look into RAID (redundant array of inexpensive disks), and how different types of RAIDs might affect the system we have. Certain RAID types have its own advantages and disadvantages. 